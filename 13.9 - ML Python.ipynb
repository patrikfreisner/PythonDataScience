{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/user/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Description</th>\n",
       "      <th>Development_Type</th>\n",
       "      <th>Dev_Cat</th>\n",
       "      <th>Affect_RPA</th>\n",
       "      <th>Dev_Sprint</th>\n",
       "      <th>Effort</th>\n",
       "      <th>Assignment_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RITM0399426</td>\n",
       "      <td>Incluir um filtro no Dashboard Automation View...</td>\n",
       "      <td>Dashboards and Reports</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>_GG_WBS_SERVICE_CENTER_SUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RITM0419622</td>\n",
       "      <td>Criar consulta para identificar oportunidades ...</td>\n",
       "      <td>Dashboards and Reports</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>_GG_WBS_SERVICE_CENTER_SUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RITM0425367</td>\n",
       "      <td>Desenvolver solução de manutenção de cadastro ...</td>\n",
       "      <td>Workflows and Tasks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>12</td>\n",
       "      <td>50.0</td>\n",
       "      <td>_GG_WBS_SERVICE_CENTER_SUPPORT_ADVANCED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RITM0429820</td>\n",
       "      <td>Hoje após o usuário solicitar um serviço no po...</td>\n",
       "      <td>Portal (Employees / Internal Users)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>_GG_WBS_SERVICE_CENTER_DEVELOPMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RITM0461779</td>\n",
       "      <td>Conforme INC0038736, houveram erros de integra...</td>\n",
       "      <td>Dashboards and Reports</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>_GG_WBS_SERVICE_CENTER_SUPPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>RITM1861929</td>\n",
       "      <td>Quando o key user usar o botão \"Move to PRD\" e...</td>\n",
       "      <td>Workflows and Tasks</td>\n",
       "      <td>Project</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>_GG_WBS_SERVICE_CENTER_DEVELOPMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>RITM1861967</td>\n",
       "      <td>Finalizar em massa as solicitações em anexo.\\n...</td>\n",
       "      <td>Agent Workspace</td>\n",
       "      <td>Support advanced</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>_GG_WBS_SERVICE_CENTER_SUPPORT_ADVANCED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>RITM1864586</td>\n",
       "      <td>Boa tarde,\\n\\nDevido ao desenvolvimento do aut...</td>\n",
       "      <td>Service Catalog</td>\n",
       "      <td>Support advanced</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>_GG_WBS_SERVICE_CENTER_SUPPORT_ADVANCED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>RITM1867199</td>\n",
       "      <td>OTC_510 esta com erro no filtro da do Multirow...</td>\n",
       "      <td>Service Catalog</td>\n",
       "      <td>Support advanced</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>_GG_WBS_SERVICE_CENTER_SUPPORT_ADVANCED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>RITM1867237</td>\n",
       "      <td>OTC_038 esta com erro no filtro da do Multirow...</td>\n",
       "      <td>Service Catalog</td>\n",
       "      <td>Support advanced</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>_GG_WBS_SERVICE_CENTER_SUPPORT_ADVANCED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3256 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Number                                        Description  \\\n",
       "0     RITM0399426  Incluir um filtro no Dashboard Automation View...   \n",
       "1     RITM0419622  Criar consulta para identificar oportunidades ...   \n",
       "2     RITM0425367  Desenvolver solução de manutenção de cadastro ...   \n",
       "3     RITM0429820  Hoje após o usuário solicitar um serviço no po...   \n",
       "4     RITM0461779  Conforme INC0038736, houveram erros de integra...   \n",
       "...           ...                                                ...   \n",
       "3251  RITM1861929  Quando o key user usar o botão \"Move to PRD\" e...   \n",
       "3252  RITM1861967  Finalizar em massa as solicitações em anexo.\\n...   \n",
       "3253  RITM1864586  Boa tarde,\\n\\nDevido ao desenvolvimento do aut...   \n",
       "3254  RITM1867199  OTC_510 esta com erro no filtro da do Multirow...   \n",
       "3255  RITM1867237  OTC_038 esta com erro no filtro da do Multirow...   \n",
       "\n",
       "                         Development_Type           Dev_Cat Affect_RPA  \\\n",
       "0                  Dashboards and Reports               NaN         No   \n",
       "1                  Dashboards and Reports               NaN         No   \n",
       "2                     Workflows and Tasks               NaN         No   \n",
       "3     Portal (Employees / Internal Users)               NaN         No   \n",
       "4                  Dashboards and Reports               NaN         No   \n",
       "...                                   ...               ...        ...   \n",
       "3251                  Workflows and Tasks           Project         No   \n",
       "3252                      Agent Workspace  Support advanced         No   \n",
       "3253                      Service Catalog  Support advanced         No   \n",
       "3254                      Service Catalog  Support advanced         No   \n",
       "3255                      Service Catalog  Support advanced         No   \n",
       "\n",
       "     Dev_Sprint  Effort                         Assignment_group  \n",
       "0           NaN     2.0           _GG_WBS_SERVICE_CENTER_SUPPORT  \n",
       "1           NaN     4.0           _GG_WBS_SERVICE_CENTER_SUPPORT  \n",
       "2            12    50.0  _GG_WBS_SERVICE_CENTER_SUPPORT_ADVANCED  \n",
       "3            12     5.0       _GG_WBS_SERVICE_CENTER_DEVELOPMENT  \n",
       "4           NaN     2.0           _GG_WBS_SERVICE_CENTER_SUPPORT  \n",
       "...         ...     ...                                      ...  \n",
       "3251        NaN     2.0       _GG_WBS_SERVICE_CENTER_DEVELOPMENT  \n",
       "3252        NaN     2.0  _GG_WBS_SERVICE_CENTER_SUPPORT_ADVANCED  \n",
       "3253        NaN     2.0  _GG_WBS_SERVICE_CENTER_SUPPORT_ADVANCED  \n",
       "3254        NaN     3.0  _GG_WBS_SERVICE_CENTER_SUPPORT_ADVANCED  \n",
       "3255        NaN     3.0  _GG_WBS_SERVICE_CENTER_SUPPORT_ADVANCED  \n",
       "\n",
       "[3256 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"./files/devsup_analysis.csv\", sep=\";\", encoding='latin-1')\n",
    "df[\"Effort\"].astype(\"float\")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/home/user/nltk_data'\n    - '/home/user/backup/PythonDataScience/env/nltk_data'\n    - '/home/user/backup/PythonDataScience/env/share/nltk_data'\n    - '/home/user/backup/PythonDataScience/env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/backup/PythonDataScience/env/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/backup/PythonDataScience/env/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - '/home/user/nltk_data'\n    - '/home/user/backup/PythonDataScience/env/nltk_data'\n    - '/home/user/backup/PythonDataScience/env/share/nltk_data'\n    - '/home/user/backup/PythonDataScience/env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Lemmatization\u001b[39;00m\n\u001b[1;32m     29\u001b[0m document \u001b[38;5;241m=\u001b[39m document\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m---> 31\u001b[0m document \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m document]\n\u001b[1;32m     32\u001b[0m document \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(document)\n\u001b[1;32m     34\u001b[0m documents\u001b[38;5;241m.\u001b[39mappend(document)\n",
      "Cell \u001b[0;32mIn [31], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Lemmatization\u001b[39;00m\n\u001b[1;32m     29\u001b[0m document \u001b[38;5;241m=\u001b[39m document\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m---> 31\u001b[0m document \u001b[38;5;241m=\u001b[39m [\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m document]\n\u001b[1;32m     32\u001b[0m document \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(document)\n\u001b[1;32m     34\u001b[0m documents\u001b[38;5;241m.\u001b[39mappend(document)\n",
      "File \u001b[0;32m~/backup/PythonDataScience/env/lib/python3.10/site-packages/nltk/stem/wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m     \u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[1;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[0;32m~/backup/PythonDataScience/env/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/backup/PythonDataScience/env/lib/python3.10/site-packages/nltk/corpus/util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__reader_cls(root, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__kwargs)\n\u001b[1;32m     91\u001b[0m \u001b[39m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m# match that of the corpus.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m args, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs\n",
      "File \u001b[0;32m~/backup/PythonDataScience/env/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[0;34m(self, root, omw_reader)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1173\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1174\u001b[0m     )\n\u001b[1;32m   1175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprovenances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49momw_prov()\n\u001b[1;32m   1178\u001b[0m \u001b[39m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang_data \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n",
      "File \u001b[0;32m~/backup/PythonDataScience/env/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m provdict \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1284\u001b[0m provdict[\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1285\u001b[0m fileids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_omw_reader\u001b[39m.\u001b[39;49mfileids()\n\u001b[1;32m   1286\u001b[0m \u001b[39mfor\u001b[39;00m fileid \u001b[39min\u001b[39;00m fileids:\n\u001b[1;32m   1287\u001b[0m     prov, langfile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplit(fileid)\n",
      "File \u001b[0;32m~/backup/PythonDataScience/env/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/backup/PythonDataScience/env/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/backup/PythonDataScience/env/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/backup/PythonDataScience/env/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/home/user/nltk_data'\n    - '/home/user/backup/PythonDataScience/env/nltk_data'\n    - '/home/user/backup/PythonDataScience/env/share/nltk_data'\n    - '/home/user/backup/PythonDataScience/env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "textArr = df[\"Description\"]\n",
    "\n",
    "documents = []\n",
    "\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(textArr)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(textArr[sen]))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "\n",
    "    documents.append(document)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78c3fd5810f68422452cc8d89406bfa1f9d96eb6340ecead556bb9d68797532a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
